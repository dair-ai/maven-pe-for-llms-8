{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Augmented Question Answering\n",
    "\n",
    "We are interested to build a personal learning assistant using LangChain. The parts we need:\n",
    "\n",
    "- user question (input)\n",
    "- role prompting to mimic learning assistant role\n",
    "- relevant context obtained via data source\n",
    "    - knowledge base/data source (we are using lecture transcriptions for simplicity)\n",
    "- vector database to store the data source and support semantic search\n",
    "- personalized response with source/citations (summarized output)\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/maven-pe-for-llms-8/blob/main/demos/session-4/rag-qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install --upgrade chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data we want to use as source to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data source, we will use a transcription of Karpathy's recent lecture on GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "with open('../data/kar-gpt.txt') as f:\n",
    "    text_data = f.read()\n",
    "    \n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \")\n",
    "texts = text_splitter.split_text(text_data)\n",
    "\n",
    "# embeddings obtained from OpenAI (you can use open-source like FAISS)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi everyone. So by now you have probably heard of chat GPT. It has taken the world and AI community by storm and it is a system that allows you to interact with an AI and give it text-based tasks. So for example, we can ask chat GPT to write us a small hi-cute about how important it is that people understand AI and then they can use it to improve the world and make it more prosperous. So when we run this, AI knowledge brings prosperity for all to see embraces power. Okay, not bad. And so you could see that chat GPT went from left to right and generated all these words see sort of sequentially. Now I asked it already the exact same prompt a little bit earlier and it generated a slightly different outcome. AI is power to grow, ignorance holds us back, learn, prosperity, weights. So pretty good in both cases and slightly different. So you can see that chat GPT is a probabilistic system and for any one prompt it can give us multiple answers sort of replying to it. Now this is just one',\n",
       " \"example of the prompt. People have come up with many, many examples and there are entire websites that index interactions with chat GPT and so many of them are quite humorous. Explain HTML to me like I'm a dog. Write reliefs notes for chest two. Write a note about Elon Musk buying a Twitter and so on. So as an example, please write a breaking news article about a leaf falling from a tree. In a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf which was previously attached to a branch of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty remarkable system and it is what we call a language model because it models the sequence of words or characters or tokens more generally and it knows how sort of words follow each other in English language. And so from its perspective, what it is doing is it is completing the sequence. So I give it the start of a sequence and it completes the sequence\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the course about?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' The course is about training language models, specifically the transformer neural network, using Python and basic understanding of calculus and statistics. It also includes a code base and notebook for training models like GPT3. \\nSOURCES: 108, 1, 107, 7'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '\\nAnswer: The course is about training language models, specifically the transformer neural network, using Python and basic understanding of calculus and statistics. It also covers fine tuning and other stages for tasks such as sentiment detection. The course includes a Google colab Jupyter notebook and code for training models similar to GPT3. (SOURCES: 1, 7, 107, 108)'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\n",
    "Given the summary above, help answer the following question from the user:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out other chains like mapreduce and refine if you are working with bigger context and larger documents. https://docs.langchain.com/docs/components/chains/index_related_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The course is about using Python and basic understanding of calculus and statistics to train language models, specifically the transformer neural network, and includes a code base and notebook for training models like GPT3.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\"],\n",
    "    template=\"\"\"You are a personal learning assistant. \n",
    "    Just take the answer from the previous response {response} and summarize it into one sentence.\n",
    "\n",
    "    Agent:\n",
    "    \"\"\"\n",
    ")\n",
    "query = \"What is the course about?\"\n",
    "response_chain = ( {\"response\": chain} | response_prompt | llm)\n",
    "response_chain.invoke({\"input_documents\": docs ,\"question\": query})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Add another chain that connects with the previous `agent_chain` to create another agent that tries to be helpful and follows up with a question if it helps to keep the conversation going."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
